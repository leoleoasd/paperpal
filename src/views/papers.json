[
    {
        "id": "aggarwalExplanationsCommonsenseQANew2021",
        "type": "inproceedings",
        "title": "Explanations for CommonsenseQA: New Dataset and Models",
        "shorttitle": "Explanations for CommonsenseQA",
        "booktitle": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
        "author": "Aggarwal, Shourya and Mandowara, Divyanshu and Agrawal, Vishwajeet and Khandelwal, Dinesh and Singla, Parag and Garg, Dinesh",
        "editor": "Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto",
        "year": "2021",
        "pages": "3050--3065",
        "publisher": "Association for Computational Linguistics",
        "address": "Online",
        "doi": "10.18653/v1/2021.acl-long.238",
        "urldate": "2023-11-07",
        "abstract": "CommonsenseQA (CQA) (Talmor et al., 2019) dataset was recently released to advance the research on common-sense question answering (QA) task. Whereas the prior work has mostly focused on proposing QA models for this dataset, our aim is to retrieve as well as generate explanation for a given (question, correct answer choice, incorrect answer choices) tuple from this dataset. Our explanation definition is based on certain desiderata, and translates an explanation into a set of positive and negative common-sense properties (aka facts) which not only explain the correct answer choice but also refute the incorrect ones. We human-annotate a first-of-its-kind dataset (called ECQA) of positive and negative properties, as well as free-flow explanations, for 11K QA pairs taken from the CQA dataset. We propose a latent representation based property retrieval model as well as a GPT-2 based property generation model with a novel two step fine-tuning procedure. We also propose a free-flow explanation generation model. Extensive experiments show that our retrieval model beats BM25 baseline by a relative gain of 100% in F_1 score, property generation model achieves a respectable F_1 score of 36.4, and free-flow generation model achieves a similarity score of 61.9, where last two scores are based on a human correlated semantic similarity metric."
    },
    {
        "id": "aguirreSelectingShotsDemographic2023",
        "type": "misc",
        "title": "Selecting Shots for Demographic Fairness in Few-Shot Learning with Large Language Models",
        "author": "Aguirre, Carlos and Sasse, Kuleen and Cachola, Isabel and Dredze, Mark",
        "year": "2023",
        "number": "arXiv:2311.08472",
        "eprint": "2311.08472",
        "primaryclass": "cs",
        "publisher": "arXiv",
        "url": "http://arxiv.org/abs/2311.08472",
        "urldate": "2023-11-18",
        "abstract": "Recently, work in NLP has shifted to few-shot (in-context) learning, with large language models (LLMs) performing well across a range of tasks. However, while fairness evaluations have become a standard for supervised methods, little is known about the fairness of LLMs as prediction systems. Further, common standard methods for fairness involve access to models weights or are applied during finetuning, which are not applicable in few-shot learning. Do LLMs exhibit prediction biases when used for standard NLP tasks? In this work, we explore the effect of shots, which directly affect the performance of models, on the fairness of LLMs as NLP classification systems. We consider how different shot selection strategies, both existing and new demographically sensitive methods, affect model fairness across three standard fairness datasets. We discuss how future work can include LLM fairness evaluations.",
        "archiveprefix": "arxiv",
        "keywords": "Computer Science - Computation and Language"
    },
    {
        "id": "chenLearningQLargeScaleDataset2018",
        "type": "article",
        "title": "LearningQ: A Large-Scale Dataset for Educational Question Generation",
        "shorttitle": "LearningQ",
        "author": "Chen, Guanliang and Yang, Jie and Hauff, Claudia and Houben, Geert-Jan",
        "year": "2018",
        "journal": "Proceedings of the International AAAI Conference on Web and Social Media",
        "volume": "12",
        "number": "1",
        "issn": "2334-0770",
        "doi": "10.1609/icwsm.v12i1.14987",
        "urldate": "2023-11-07",
        "abstract": "We present LearningQ, a challenging educational question generation dataset containing over 230K document-question pairs. It includes 7K instructor-designed questions assessing knowledge concepts being taught and 223K learner-generated questions seeking in-depth understanding of the taught concepts. We show that, compared to existing datasets that can be used to generate educational questions, LearningQ (i) covers a wide range of educational topics and (ii) contains long and cognitively demanding documents for which question generation requires reasoning over the relationships between sentences and paragraphs. As a result, a significant percentage of LearningQ questions (textasciitilde 30%) require higher-order cognitive skills to solve (such as applying, analyzing), in contrast to existing question-generation datasets that are designed mostly for the lowest cognitive skill level (i.e. remembering). To understand the effectiveness of existing question generation methods in producing educational questions, we evaluate both rule-based and deep neural network based methods on LearningQ. Extensive experiments show that state-of-the-art methods which perform well on existing datasets cannot generate useful educational questions. This implies that LearningQ is a challenging test bed for the generation of high-quality educational questions and worth further investigation. We open-source the dataset and our codes at https://dataverse.mpi-sws.org/dataverse/icwsm18.",
        "copyright": "Copyright (c) 2022 Proceedings of the International AAAI Conference on Web and Social Media",
        "langid": "english",
        "keywords": "Bloom's Revised Taxonomy"
    },
    {
        "id": "gilardiChatGPTOutperformsCrowdWorkers2023b",
        "type": "article",
        "title": "ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks",
        "author": "Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Ma\\\"el",
        "year": "2023",
        "journal": "Proceedings of the National Academy of Sciences",
        "volume": "120",
        "number": "30",
        "eprint": "2303.15056",
        "primaryclass": "cs",
        "pages": "e2305016120",
        "issn": "0027-8424, 1091-6490",
        "doi": "10.1073/pnas.2305016120",
        "urldate": "2023-11-13",
        "abstract": "Many NLP applications require manual data annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using a sample of 2,382 tweets, we demonstrate that ChatGPT outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frames detection. Specifically, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks, while ChatGPT's intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than $0.003 -- about twenty times cheaper than MTurk. These results show the potential of large language models to drastically increase the efficiency of text classification.",
        "archiveprefix": "arxiv",
        "keywords": "Computer Science - Computation and Language,Computer Science - Computers and Society"
    },
    {
        "id": "kuzmanChatGPTBeginningEnd2023",
        "type": "misc",
        "title": "ChatGPT: Beginning of an End of Manual Linguistic Data Annotation? Use Case of Automatic Genre Identification",
        "shorttitle": "ChatGPT",
        "author": "Kuzman, Taja and Mozetiv c, Igor and Ljubev si'c, Nikola",
        "year": "2023",
        "number": "arXiv:2303.03953",
        "eprint": "2303.03953",
        "primaryclass": "cs",
        "publisher": "arXiv",
        "url": "http://arxiv.org/abs/2303.03953",
        "urldate": "2023-11-13",
        "abstract": "ChatGPT has shown strong capabilities in natural language generation tasks, which naturally leads researchers to explore where its abilities end. In this paper, we examine whether ChatGPT can be used for zero-shot text classification, more specifically, automatic genre identification. We compare ChatGPT with a multilingual XLM-RoBERTa language model that was fine-tuned on datasets, manually annotated with genres. The models are compared on test sets in two languages: English and Slovenian. Results show that ChatGPT outperforms the fine-tuned model when applied to the dataset which was not seen before by either of the models. Even when applied on Slovenian language as an under-resourced language, ChatGPT's performance is no worse than when applied to English. However, if the model is fully prompted in Slovenian, the performance drops significantly, showing the current limitations of ChatGPT usage on smaller languages. The presented results lead us to questioning whether this is the beginning of an end of laborious manual annotation campaigns even for smaller languages, such as Slovenian.",
        "archiveprefix": "arxiv",
        "keywords": "Computer Science - Computation and Language,Computer Science - Machine Learning"
    },
    {
        "id": "lippiCLAUDETTEAutomatedDetector2019a",
        "type": "misc",
        "title": "CLAUDETTE: An Automated Detector of Potentially Unfair Clauses in Online Terms of Service",
        "shorttitle": "CLAUDETTE",
        "author": "Lippi, Marco and Palka, Przemyslaw and Contissa, Giuseppe and Lagioia, Francesca and Micklitz, Hans-Wolfgang and Sartor, Giovanni and Torroni, Paolo",
        "year": "2019",
        "eprint": "1805.01217",
        "primaryclass": "cs",
        "doi": "10.1007/s10506-019-09243-2",
        "urldate": "2023-11-01",
        "abstract": "Terms of service of on-line platforms too often contain clauses that are potentially unfair to the consumer. We present an experimental study where machine learning is employed to automatically detect such potentially unfair clauses. Results show that the proposed system could provide a valuable tool for lawyers and consumers alike.",
        "archiveprefix": "arxiv",
        "keywords": "Computer Science - Artificial Intelligence,Computer Science - Computers and Society"
    },
    {
        "id": "liuGEvalNLGEvaluation2023",
        "type": "misc",
        "title": "G-Eval: NLG Evaluation Using GPT-4 with Better Human Alignment",
        "shorttitle": "G-Eval",
        "author": "Liu, Yang and Iter, Dan and Xu, Yichong and Wang, Shuohang and Xu, Ruochen and Zhu, Chenguang",
        "year": "2023",
        "number": "arXiv:2303.16634",
        "eprint": "2303.16634",
        "primaryclass": "cs",
        "publisher": "arXiv",
        "doi": "10.48550/arXiv.2303.16634",
        "urldate": "2023-11-13",
        "abstract": "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval",
        "archiveprefix": "arxiv",
        "keywords": "Computer Science - Artificial Intelligence,Computer Science - Computation and Language"
    },
    {
        "id": "schroderRevisitingUncertaintybasedQuery2022",
        "type": "inproceedings",
        "title": "Revisiting Uncertainty-based Query Strategies for Active Learning with Transformers",
        "booktitle": "Findings of the Association for Computational Linguistics: ACL 2022",
        "author": "Schr\\\"oder, Christopher and Niekler, Andreas and Potthast, Martin",
        "year": "2022",
        "pages": "2194--2203",
        "publisher": "Association for Computational Linguistics",
        "address": "Dublin, Ireland",
        "doi": "10.18653/v1/2022.findings-acl.172",
        "urldate": "2023-11-16",
        "abstract": "Active learning is the iterative construction of a classification model through targeted labeling, enabling significant labeling cost savings. As most research on active learning has been carried out before transformer-based language models (``Ztransformers'') became popular, despite its practical importance, comparably few papers have investigated how transformers can be combined with active learning to date. This can be attributed to the fact that using state-of-the-art query strategies for transformers induces a prohibitive runtime overhead, which effectively nullifies, or even outweighs the desired cost savings. For this reason, we revisit uncertainty-based query strategies, which had been largely outperformed before, but are particularly suited in the context of fine-tuning transformers. In an extensive evaluation, we connect transformers to experiments from previous research, assessing their performance on five widely used text classification benchmarks. For active learning with transformers, several other uncertainty-based approaches outperform the well-known prediction entropy query strategy, thereby challenging its status as most popular uncertainty baseline in active learning for text classification.",
        "langid": "english"
    },
    {
        "id": "senerActiveLearningConvolutional2018",
        "type": "inproceedings",
        "title": "Active Learning for Convolutional Neural Networks: A Core-Set Approach",
        "shorttitle": "Active Learning for Convolutional Neural Networks",
        "booktitle": "International Conference on Learning Representations",
        "author": "Sener, Ozan and Savarese, Silvio",
        "year": "2018",
        "url": "https://openreview.net/forum?id=H1aIuk-RW",
        "urldate": "2023-11-16",
        "abstract": "Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (i.e. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs when applied in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.",
        "langid": "english"
    },
    {
        "id": "sharmaActiveLearningRationales2015",
        "type": "inproceedings",
        "title": "Active Learning with Rationales for Text Classification",
        "booktitle": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
        "author": "Sharma, Manali and Zhuang, Di and Bilgic, Mustafa",
        "editor": "Mihalcea, Rada and Chai, Joyce and Sarkar, Anoop",
        "year": "2015",
        "pages": "441--451",
        "publisher": "Association for Computational Linguistics",
        "address": "Denver, Colorado",
        "doi": "10.3115/v1/N15-1047",
        "urldate": "2023-11-13"
    },
    {
        "id": "wangMiniLMDeepSelfAttention2020",
        "type": "inproceedings",
        "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers",
        "shorttitle": "MiniLM",
        "booktitle": "Advances in Neural Information Processing Systems",
        "author": "Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming",
        "year": "2020",
        "volume": "33",
        "pages": "5776--5788",
        "publisher": "Curran Associates, Inc.",
        "url": "https://proceedings.neurips.cc/paper/2020/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html",
        "urldate": "2023-11-14",
        "abstract": "Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models."
    },
    {
        "id": "weiFinetunedLanguageModels2021",
        "type": "inproceedings",
        "title": "Finetuned Language Models Are Zero-Shot Learners",
        "booktitle": "International Conference on Learning Representations",
        "author": "Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.",
        "year": "2021",
        "url": "https://openreview.net/forum?id=gEZrGCozdqR",
        "urldate": "2023-11-14",
        "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuningtextemdash finetuning language models on a collection of datasets described via instructionstextemdash substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",
        "langid": "english",
        "keywords": "Computer Science - Computation and Language"
    },
    {
        "id": "xuFantasticQuestionsWhere2022",
        "type": "misc",
        "title": "Fantastic Questions and Where to Find Them: FairytaleQA -- An Authentic Dataset for Narrative Comprehension",
        "shorttitle": "Fantastic Questions and Where to Find Them",
        "author": "Xu, Ying and Wang, Dakuo and Yu, Mo and Ritchie, Daniel and Yao, Bingsheng and Wu, Tongshuang and Zhang, Zheng and Li, Toby Jia-Jun and Bradford, Nora and Sun, Branda and Hoang, Tran Bao and Sang, Yisi and Hou, Yufang and Ma, Xiaojuan and Yang, Diyi and Peng, Nanyun and Yu, Zhou and Warschauer, Mark",
        "year": "2022",
        "number": "arXiv:2203.13947",
        "eprint": "2203.13947",
        "primaryclass": "cs",
        "publisher": "arXiv",
        "url": "http://arxiv.org/abs/2203.13947",
        "urldate": "2023-11-08",
        "abstract": "Question answering (QA) is a fundamental means to facilitate assessment and training of narrative comprehension skills for both machines and young children, yet there is scarcity of high-quality QA datasets carefully designed to serve this purpose. In particular, existing datasets rarely distinguish fine-grained reading skills, such as the understanding of varying narrative elements. Drawing on the reading education research, we introduce FairytaleQA, a dataset focusing on narrative comprehension of kindergarten to eighth-grade students. Generated by educational experts based on an evidence-based theoretical framework, FairytaleQA consists of 10,580 explicit and implicit questions derived from 278 children-friendly stories, covering seven types of narrative elements or relations. Our dataset is valuable in two folds: First, we ran existing QA models on our dataset and confirmed that this annotation helps assess models' fine-grained learning skills. Second, the dataset supports question generation (QG) task in the education domain. Through benchmarking with QG models, we show that the QG model trained on FairytaleQA is capable of asking high-quality and more diverse questions.",
        "archiveprefix": "arxiv",
        "keywords": "Computer Science - Computation and Language"
    },
    {
        "id": "xuMentalLLMLeveragingLarge2023a",
        "type": "misc",
        "title": "Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data",
        "shorttitle": "Mental-LLM",
        "author": "Xu, Xuhai and Yao, Bingsheng and Dong, Yuanzhe and Gabriel, Saadia and Yu, Hong and Hendler, James and Ghassemi, Marzyeh and Dey, Anind K. and Wang, Dakuo",
        "year": "2023",
        "number": "arXiv:2307.14385",
        "eprint": "2307.14385",
        "primaryclass": "cs",
        "publisher": "arXiv",
        "doi": "10.48550/arXiv.2307.14385",
        "urldate": "2023-10-31",
        "abstract": "Advances in large language models (LLMs) have empowered a variety of applications. However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health. In this work, we present the first comprehensive evaluation of multiple LLMs, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4, on various mental health prediction tasks via online text data. We conduct a broad range of experiments, covering zero-shot prompting, few-shot prompting, and instruction fine-tuning. The results indicate a promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for the mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9% on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8%. They further perform on par with the state-of-the-art task-specific language model. We also conduct an exploratory case study on LLMs' capability on the mental health reasoning tasks, illustrating the promising capability of certain models such as GPT-4. We summarize our findings into a set of action guidelines for potential methods to enhance LLMs' capability for mental health tasks. Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias. We highlight the important ethical risks accompanying this line of research.",
        "archiveprefix": "arxiv",
        "keywords": "68U35,Computer Science - Computation and Language,H.5.2,I.2.m"
    },
    {
        "id": "yaoLabelsEmpoweringHuman2023",
        "type": "misc",
        "title": "Beyond Labels: Empowering Human Annotators with Natural Language Explanations through a Novel Active-Learning Architecture",
        "shorttitle": "Beyond Labels",
        "author": "Yao, Bingsheng and Jindal, Ishan and Popa, Lucian and Katsis, Yannis and Ghosh, Sayan and He, Lihong and Lu, Yuxuan and Srivastava, Shashank and Li, Yunyao and Hendler, James and Wang, Dakuo",
        "year": "2023",
        "number": "arXiv:2305.12710",
        "eprint": "2305.12710",
        "primaryclass": "cs",
        "publisher": "arXiv",
        "doi": "10.48550/arXiv.2305.12710",
        "urldate": "2023-11-13",
        "abstract": "Real-world domain experts (e.g., doctors) rarely annotate only a decision label in their day-to-day workflow without providing explanations. Yet, existing low-resource learning techniques, such as Active Learning (AL), that aim to support human annotators mostly focus on the label while neglecting the natural language explanation of a data point. This work proposes a novel AL architecture to support experts' real-world need for label and explanation annotations in low-resource scenarios. Our AL architecture leverages an explanation-generation model to produce explanations guided by human explanations, a prediction model that utilizes generated explanations toward prediction faithfully, and a novel data diversity-based AL sampling strategy that benefits from the explanation annotations. Automated and human evaluations demonstrate the effectiveness of incorporating explanations into AL sampling and the improved human annotation efficiency and trustworthiness with our AL architecture. Additional ablation studies illustrate the potential of our AL architecture for transfer learning, generalizability, and integration with large language models (LLMs). While LLMs exhibit exceptional explanation-generation capabilities for relatively simple tasks, their effectiveness in complex real-world tasks warrants further in-depth study.",
        "archiveprefix": "arxiv",
        "keywords": "Computer Science - Computation and Language"
    }
]